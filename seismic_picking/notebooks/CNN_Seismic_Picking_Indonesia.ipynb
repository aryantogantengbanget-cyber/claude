{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸŒ Auto Picking Gelombang Seismik P/S dengan CNN\n",
    "## Dataset Seismik Indonesia\n",
    "\n",
    "---\n",
    "\n",
    "**Dibuat untuk:** Analisis seismik otomatis dengan Deep Learning  \n",
    "**Model:** Convolutional Neural Network (CNN) 2D  \n",
    "**Dataset:** CSV seismogram Indonesia dengan augmentasi data  \n",
    "**Output:** Deteksi otomatis waktu kedatangan gelombang P dan S\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“‹ Daftar Isi:\n",
    "1. Setup dan Instalasi\n",
    "2. Persiapan Dataset\n",
    "3. Eksplorasi dan Visualisasi Data\n",
    "4. Arsitektur Model CNN\n",
    "5. Training dengan Augmentasi\n",
    "6. Evaluasi dan Validasi\n",
    "7. Testing pada Data Baru\n",
    "8. Export Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1ï¸âƒ£ Setup dan Instalasi Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow pandas numpy matplotlib scipy scikit-learn seaborn\n",
    "\n",
    "print(\"âœ… Semua dependencies berhasil diinstall!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from datetime import datetime\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU tersedia: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"\\nâœ… Import berhasil!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone"
   },
   "source": [
    "### Clone Repository (Opsional)\n",
    "\n",
    "Jika Anda sudah memiliki repository dengan kode ini, clone di sini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone repository (ganti dengan URL repository Anda)\n",
    "# !git clone https://github.com/username/seismic-picking.git\n",
    "# %cd seismic-picking\n",
    "\n",
    "# Atau upload file secara manual\n",
    "print(\"ðŸ“ Siap untuk upload dataset CSV Anda!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount"
   },
   "source": [
    "### Mount Google Drive (untuk menyimpan dataset dan model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set working directory\n",
    "WORK_DIR = '/content/seismic_picking'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_prep"
   },
   "source": [
    "## 2ï¸âƒ£ Persiapan Dataset\n",
    "\n",
    "### Upload Dataset CSV Anda\n",
    "\n",
    "Format CSV yang diharapkan:\n",
    "- Kolom: `time`, `Z`, `N`, `E`, `p_arrival`, `s_arrival`\n",
    "- Atau: `time`, `amplitude`, `p_arrival`, `s_arrival`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload"
   },
   "outputs": [],
   "source": [
    "# Upload dataset CSV\n",
    "from google.colab import files\n",
    "\n",
    "# Buat direktori dataset\n",
    "DATASET_DIR = 'dataset'\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Upload file CSV dataset Anda (bisa multiple files):\")\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# Uncomment untuk upload\n",
    "# for filename in uploaded.keys():\n",
    "#     os.rename(filename, os.path.join(DATASET_DIR, filename))\n",
    "#     print(f\"âœ… {filename} berhasil di-upload\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Alternatif: Kita akan generate synthetic data untuk demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "synthetic"
   },
   "source": [
    "### Generate Synthetic Data (untuk demo/testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gen_synthetic"
   },
   "outputs": [],
   "source": [
    "# Generate synthetic seismic data\n",
    "class SyntheticDataGenerator:\n",
    "    def __init__(self, sampling_rate=100):\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def generate_synthetic_waveform(self, duration=30, p_time=8.74, s_time=14.69):\n",
    "        n_samples = int(duration * self.sampling_rate)\n",
    "        time = np.linspace(0, duration, n_samples)\n",
    "        waveform = np.zeros((n_samples, 3))\n",
    "        \n",
    "        # Add noise\n",
    "        waveform += np.random.normal(0, 0.1, (n_samples, 3))\n",
    "        \n",
    "        # P-wave\n",
    "        p_idx = int(p_time * self.sampling_rate)\n",
    "        p_duration = int(3 * self.sampling_rate)\n",
    "        p_envelope = signal.windows.tukey(p_duration, alpha=0.5)\n",
    "        p_signal = p_envelope * np.sin(2 * np.pi * 8 * np.linspace(0, 3, p_duration))\n",
    "        waveform[p_idx:p_idx+len(p_signal), 0] += p_signal * 0.3\n",
    "        \n",
    "        # S-wave\n",
    "        s_idx = int(s_time * self.sampling_rate)\n",
    "        s_duration = int(8 * self.sampling_rate)\n",
    "        s_envelope = signal.windows.tukey(s_duration, alpha=0.3)\n",
    "        s_signal = s_envelope * np.sin(2 * np.pi * 4 * np.linspace(0, 8, s_duration))\n",
    "        waveform[s_idx:s_idx+len(s_signal), :] += s_signal[:, None] * np.array([0.5, 0.7, 0.7])\n",
    "        \n",
    "        return time, waveform, p_idx, s_idx\n",
    "\n",
    "    def save_synthetic_csv(self, output_dir, n_samples=200):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for i in range(n_samples):\n",
    "            p_time = np.random.uniform(5, 15)\n",
    "            s_time = p_time + np.random.uniform(3, 10)\n",
    "            duration = max(30, s_time + 10)\n",
    "            \n",
    "            time, waveform, p_idx, s_idx = self.generate_synthetic_waveform(\n",
    "                duration=duration, p_time=p_time, s_time=s_time\n",
    "            )\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'time': time,\n",
    "                'Z': waveform[:, 0],\n",
    "                'N': waveform[:, 1],\n",
    "                'E': waveform[:, 2],\n",
    "                'p_arrival': p_idx,\n",
    "                's_arrival': s_idx\n",
    "            })\n",
    "            \n",
    "            filename = os.path.join(output_dir, f'synthetic_event_{i:04d}.csv')\n",
    "            df.to_csv(filename, index=False)\n",
    "        \n",
    "        print(f\"âœ… Generated {n_samples} synthetic seismograms\")\n",
    "\n",
    "# Generate dataset\n",
    "generator = SyntheticDataGenerator(sampling_rate=100)\n",
    "generator.save_synthetic_csv(DATASET_DIR, n_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore"
   },
   "source": [
    "## 3ï¸âƒ£ Eksplorasi dan Visualisasi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_sample"
   },
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "import glob\n",
    "\n",
    "csv_files = glob.glob(os.path.join(DATASET_DIR, '*.csv'))\n",
    "print(f\"ðŸ“Š Jumlah file CSV: {len(csv_files)}\")\n",
    "\n",
    "# Load first file\n",
    "if csv_files:\n",
    "    sample_df = pd.read_csv(csv_files[0])\n",
    "    print(f\"\\nðŸ“„ Sample file: {os.path.basename(csv_files[0])}\")\n",
    "    print(f\"Shape: {sample_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(sample_df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(sample_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize"
   },
   "outputs": [],
   "source": [
    "# Visualize sample waveform with STA/LTA detection\n",
    "\n",
    "def compute_sta_lta(waveform, sta_window=0.5, lta_window=5.0, sampling_rate=100):\n",
    "    nsta = int(sta_window * sampling_rate)\n",
    "    nlta = int(lta_window * sampling_rate)\n",
    "    waveform_sq = waveform ** 2\n",
    "    sta = np.convolve(waveform_sq, np.ones(nsta) / nsta, mode='same')\n",
    "    lta = np.convolve(waveform_sq, np.ones(nlta) / nlta, mode='same')\n",
    "    sta_lta = np.divide(sta, lta, where=lta != 0)\n",
    "    return sta_lta\n",
    "\n",
    "# Get waveform data\n",
    "waveform_z = sample_df['Z'].values\n",
    "time = sample_df['time'].values\n",
    "p_arrival = sample_df['p_arrival'].iloc[0]\n",
    "s_arrival = sample_df['s_arrival'].iloc[0]\n",
    "\n",
    "# Compute STA/LTA\n",
    "sta_lta_p = compute_sta_lta(waveform_z, sta_window=0.5, lta_window=5.0)\n",
    "sta_lta_s = compute_sta_lta(waveform_z, sta_window=1.0, lta_window=10.0)\n",
    "\n",
    "# Calculate S-P time\n",
    "sp_time = (s_arrival - p_arrival) / 100  # sampling rate = 100\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "# Waveform\n",
    "axes[0].plot(time, waveform_z, 'k-', linewidth=0.7, label='Waveform')\n",
    "axes[0].axvline(p_arrival/100, color='blue', linestyle='--', linewidth=2.5, label=f'P: {p_arrival/100:.2f}s')\n",
    "axes[0].axvline(s_arrival/100, color='red', linestyle='--', linewidth=2.5, label=f'S: {s_arrival/100:.2f}s')\n",
    "axes[0].set_ylabel('Amplitude', fontweight='bold')\n",
    "axes[0].set_title('Seismic Waveform', fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# P-wave detection\n",
    "axes[1].plot(time, sta_lta_p, 'b-', linewidth=1.2)\n",
    "axes[1].axhline(4.0, color='gray', linestyle=':', linewidth=1.5)\n",
    "axes[1].axvline(p_arrival/100, color='blue', linestyle='--', linewidth=2.5)\n",
    "axes[1].set_ylabel('STA/LTA', fontweight='bold')\n",
    "axes[1].set_title('P-wave Detection', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# S-wave detection\n",
    "axes[2].plot(time, sta_lta_s, 'r-', linewidth=1.2)\n",
    "axes[2].axhline(2.5, color='gray', linestyle=':', linewidth=1.5)\n",
    "axes[2].axvline(s_arrival/100, color='red', linestyle='--', linewidth=2.5)\n",
    "axes[2].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[2].set_ylabel('STA/LTA', fontweight='bold')\n",
    "axes[2].set_title('S-wave Detection', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Synthetic Earthquake Test | S-P: {sp_time:.2f}s', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ P-arrival: {p_arrival/100:.2f}s\")\n",
    "print(f\"ðŸ“ˆ S-arrival: {s_arrival/100:.2f}s\")\n",
    "print(f\"ðŸ“ˆ S-P time: {sp_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loader"
   },
   "source": [
    "## 4ï¸âƒ£ Data Loading dan Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_class"
   },
   "outputs": [],
   "source": [
    "# Data loader class\n",
    "class SeismicDataLoader:\n",
    "    def __init__(self, data_dir, sampling_rate=100, window_size=30):\n",
    "        self.data_dir = data_dir\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.window_size = window_size\n",
    "        self.n_samples = int(sampling_rate * window_size)\n",
    "\n",
    "    def load_csv_file(self, filepath):\n",
    "        df = pd.read_csv(filepath)\n",
    "        waveform = df[['Z', 'N', 'E']].values\n",
    "        p_arrival = df['p_arrival'].iloc[0]\n",
    "        s_arrival = df['s_arrival'].iloc[0]\n",
    "        return waveform, p_arrival, s_arrival\n",
    "\n",
    "    def preprocess_waveform(self, waveform):\n",
    "        processed = waveform.copy()\n",
    "        # Detrend\n",
    "        for i in range(processed.shape[1]):\n",
    "            processed[:, i] = signal.detrend(processed[:, i])\n",
    "        # Bandpass filter (1-20 Hz)\n",
    "        nyquist = self.sampling_rate / 2\n",
    "        b, a = signal.butter(4, [1.0/nyquist, 20.0/nyquist], btype='band')\n",
    "        for i in range(processed.shape[1]):\n",
    "            processed[:, i] = signal.filtfilt(b, a, processed[:, i])\n",
    "        # Normalize\n",
    "        for i in range(processed.shape[1]):\n",
    "            max_val = np.max(np.abs(processed[:, i]))\n",
    "            if max_val > 0:\n",
    "                processed[:, i] = processed[:, i] / max_val\n",
    "        return processed\n",
    "\n",
    "    def create_windows(self, waveform, p_arrival, s_arrival, overlap=0.5):\n",
    "        windows = []\n",
    "        labels = []\n",
    "        step = int(self.n_samples * (1 - overlap))\n",
    "        \n",
    "        for start in range(0, len(waveform) - self.n_samples + 1, step):\n",
    "            end = start + self.n_samples\n",
    "            window = waveform[start:end, :]\n",
    "            window_center = (start + end) / 2\n",
    "            \n",
    "            if abs(window_center - p_arrival) < self.n_samples / 4:\n",
    "                label = 1  # P-wave\n",
    "            elif abs(window_center - s_arrival) < self.n_samples / 4:\n",
    "                label = 2  # S-wave\n",
    "            else:\n",
    "                label = 0  # Noise\n",
    "            \n",
    "            windows.append(window)\n",
    "            labels.append(label)\n",
    "        \n",
    "        return np.array(windows), np.array(labels)\n",
    "\n",
    "    def load_dataset(self, max_files=None):\n",
    "        csv_files = glob.glob(os.path.join(self.data_dir, '*.csv'))\n",
    "        if max_files:\n",
    "            csv_files = csv_files[:max_files]\n",
    "        \n",
    "        all_windows = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for idx, filepath in enumerate(csv_files):\n",
    "            if idx % 50 == 0:\n",
    "                print(f\"Processing file {idx + 1}/{len(csv_files)}\")\n",
    "            \n",
    "            waveform, p_arrival, s_arrival = self.load_csv_file(filepath)\n",
    "            waveform = self.preprocess_waveform(waveform)\n",
    "            windows, labels = self.create_windows(waveform, p_arrival, s_arrival)\n",
    "            \n",
    "            all_windows.extend(windows)\n",
    "            all_labels.extend(labels)\n",
    "        \n",
    "        X = np.array(all_windows)\n",
    "        y = np.array(all_labels)\n",
    "        \n",
    "        print(f\"\\nâœ… Dataset loaded: X={X.shape}, y={y.shape}\")\n",
    "        return X, y\n",
    "\n",
    "# Load dataset\n",
    "loader = SeismicDataLoader(DATASET_DIR, sampling_rate=100, window_size=30)\n",
    "X, y = loader.load_dataset(max_files=200)\n",
    "\n",
    "print(f\"\\nðŸ“Š Class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    cls_name = ['Noise', 'P-wave', 'S-wave'][int(cls)]\n",
    "    print(f\"  {cls_name}: {count} ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_data"
   },
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Reshape for CNN\n",
    "X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "y_cat = to_categorical(y, num_classes=3)\n",
    "\n",
    "# Split data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.125, random_state=42)\n",
    "\n",
    "print(f\"âœ… Data prepared:\")\n",
    "print(f\"   Training: {X_train.shape}\")\n",
    "print(f\"   Validation: {X_val.shape}\")\n",
    "print(f\"   Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model"
   },
   "source": [
    "## 5ï¸âƒ£ Arsitektur Model CNN 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "build_model"
   },
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def build_seismic_cnn(input_shape=(3000, 3, 1), num_classes=3):\n",
    "    inputs = keras.Input(shape=input_shape, name='seismic_input')\n",
    "    \n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, (7, 3), padding='same', activation='relu', name='conv1')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='pool1')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, (5, 3), padding='same', activation='relu', name='conv2')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='pool2')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu', name='conv3')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='pool3')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Block 4\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu', name='conv4')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D((2, 1), name='pool4')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Attention\n",
    "    attention = layers.Conv2D(1, (1, 1), activation='sigmoid', name='attention')(x)\n",
    "    x = layers.Multiply()([x, attention])\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.GlobalAveragePooling2D(name='global_pool')(x)\n",
    "    x = layers.Dense(512, activation='relu', name='dense1')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(256, activation='relu', name='dense2')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='SeismicCNN_Picker')\n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_seismic_cnn(input_shape=X_train.shape[1:])\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"âœ… Model built successfully!\\n\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "augmentation"
   },
   "source": [
    "## 6ï¸âƒ£ Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aug_class"
   },
   "outputs": [],
   "source": [
    "# Data augmentation class\n",
    "class SeismicAugmentor:\n",
    "    def __init__(self, prob=0.5):\n",
    "        self.prob = prob\n",
    "    \n",
    "    def add_noise(self, waveform, noise_level=0.05):\n",
    "        if np.random.random() < self.prob:\n",
    "            noise = np.random.normal(0, noise_level, waveform.shape)\n",
    "            return waveform + noise\n",
    "        return waveform\n",
    "    \n",
    "    def amplitude_scaling(self, waveform, scale_range=(0.8, 1.2)):\n",
    "        if np.random.random() < self.prob:\n",
    "            scale = np.random.uniform(*scale_range)\n",
    "            return waveform * scale\n",
    "        return waveform\n",
    "    \n",
    "    def time_shift(self, waveform, max_shift=50):\n",
    "        if np.random.random() < self.prob:\n",
    "            shift = np.random.randint(-max_shift, max_shift)\n",
    "            return np.roll(waveform, shift, axis=0)\n",
    "        return waveform\n",
    "    \n",
    "    def apply_all(self, waveform):\n",
    "        x = self.add_noise(waveform)\n",
    "        x = self.amplitude_scaling(x)\n",
    "        x = self.time_shift(x)\n",
    "        return x\n",
    "\n",
    "# Data generator with augmentation\n",
    "class AugmentedGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, augmentor=None, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.augmentor = augmentor\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(X))\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.X) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indices = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        X_batch = self.X[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        if self.augmentor is not None:\n",
    "            X_augmented = []\n",
    "            for x in X_batch:\n",
    "                x_squeeze = np.squeeze(x, axis=-1)\n",
    "                x_aug = self.augmentor.apply_all(x_squeeze)\n",
    "                x_aug = np.expand_dims(x_aug, axis=-1)\n",
    "                X_augmented.append(x_aug)\n",
    "            X_batch = np.array(X_augmented)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "print(\"âœ… Data augmentation ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 7ï¸âƒ£ Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train"
   },
   "outputs": [],
   "source": [
    "# Setup callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "# Create data generators\n",
    "augmentor = SeismicAugmentor(prob=0.5)\n",
    "train_gen = AugmentedGenerator(X_train, y_train, batch_size=32, augmentor=augmentor, shuffle=True)\n",
    "\n",
    "# Train model\n",
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('Model Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 8ï¸âƒ£ Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate"
   },
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"ðŸ“Š Evaluating model on test set...\\n\")\n",
    "results = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"\\nðŸ“ˆ Test Results:\")\n",
    "for metric_name, value in zip(model.metrics_names, results):\n",
    "    print(f\"  {metric_name}: {value:.4f}\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nðŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=['Noise', 'P-wave', 'S-wave']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "confusion"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Noise', 'P-wave', 'S-wave'],\n",
    "           yticklabels=['Noise', 'P-wave', 'S-wave'])\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing"
   },
   "source": [
    "## 9ï¸âƒ£ Testing pada Data Baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_new"
   },
   "outputs": [],
   "source": [
    "# Generate and test on new synthetic data\n",
    "print(\"ðŸ§ª Testing on new synthetic waveform...\\n\")\n",
    "\n",
    "generator = SyntheticDataGenerator(sampling_rate=100)\n",
    "time, waveform, p_idx, s_idx = generator.generate_synthetic_waveform(\n",
    "    duration=30, p_time=8.74, s_time=14.69\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "waveform_processed = loader.preprocess_waveform(waveform)\n",
    "\n",
    "# Create visualization\n",
    "sta_lta_p = compute_sta_lta(waveform_processed[:, 0], sta_window=0.5, lta_window=5.0)\n",
    "sta_lta_s = compute_sta_lta(waveform_processed[:, 0], sta_window=1.0, lta_window=10.0)\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "# Waveform\n",
    "axes[0].plot(time, waveform_processed[:, 0], 'k-', linewidth=0.7, label='Waveform')\n",
    "axes[0].axvline(p_idx/100, color='blue', linestyle='--', linewidth=2.5, label=f'P: {p_idx/100:.2f}s')\n",
    "axes[0].axvline(s_idx/100, color='red', linestyle='--', linewidth=2.5, label=f'S: {s_idx/100:.2f}s')\n",
    "axes[0].set_ylabel('Amplitude', fontweight='bold')\n",
    "axes[0].set_title('Seismic Waveform', fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# P-wave detection\n",
    "axes[1].plot(time, sta_lta_p, 'b-', linewidth=1.2)\n",
    "axes[1].axhline(4.0, color='gray', linestyle=':', linewidth=1.5)\n",
    "axes[1].axvline(p_idx/100, color='blue', linestyle='--', linewidth=2.5)\n",
    "axes[1].set_ylabel('STA/LTA', fontweight='bold')\n",
    "axes[1].set_title('P-wave Detection', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 20])\n",
    "\n",
    "# S-wave detection\n",
    "axes[2].plot(time, sta_lta_s, 'r-', linewidth=1.2)\n",
    "axes[2].axhline(2.5, color='gray', linestyle=':', linewidth=1.5)\n",
    "axes[2].axvline(s_idx/100, color='red', linestyle='--', linewidth=2.5)\n",
    "axes[2].set_xlabel('Time (s)', fontweight='bold')\n",
    "axes[2].set_ylabel('STA/LTA', fontweight='bold')\n",
    "axes[2].set_title('S-wave Detection', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim([0, 15])\n",
    "\n",
    "plt.suptitle('Synthetic Earthquake Test | S-P: 5.95s', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_waveform_detection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization saved as 'test_waveform_detection.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## ðŸ”Ÿ Save Model dan Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_model"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('seismic_cnn_picker_final.h5')\n",
    "print(\"âœ… Model saved as 'seismic_cnn_picker_final.h5'\")\n",
    "\n",
    "# Save to Google Drive\n",
    "!cp seismic_cnn_picker_final.h5 /content/drive/MyDrive/\n",
    "!cp best_model.h5 /content/drive/MyDrive/\n",
    "!cp training_history.png /content/drive/MyDrive/\n",
    "!cp confusion_matrix.png /content/drive/MyDrive/\n",
    "!cp test_waveform_detection.png /content/drive/MyDrive/\n",
    "\n",
    "print(\"\\nâœ… All files backed up to Google Drive!\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'model_type': 'CNN 2D',\n",
    "    'input_shape': list(X_train.shape[1:]),\n",
    "    'num_classes': 3,\n",
    "    'classes': ['Noise', 'P-wave', 'S-wave'],\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'n_train_samples': len(X_train),\n",
    "    'n_val_samples': len(X_val),\n",
    "    'n_test_samples': len(X_test),\n",
    "    'test_accuracy': float(results[1]),\n",
    "    'test_loss': float(results[0])\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Metadata saved!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ‰ SELESAI! Semua file berhasil disimpan\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## ðŸ’¡ Cara Menggunakan Model untuk Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inference_code"
   },
   "outputs": [],
   "source": [
    "# Example: Load model dan prediksi\n",
    "def predict_seismic_phases(waveform_csv_path, model_path='best_model.h5'):\n",
    "    \"\"\"\n",
    "    Predict P and S wave arrivals from seismic waveform CSV\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load and preprocess waveform\n",
    "    loader = SeismicDataLoader('.', sampling_rate=100, window_size=30)\n",
    "    df = pd.read_csv(waveform_csv_path)\n",
    "    waveform = df[['Z', 'N', 'E']].values\n",
    "    waveform_processed = loader.preprocess_waveform(waveform)\n",
    "    \n",
    "    # Create windows\n",
    "    windows, _ = loader.create_windows(waveform_processed, 0, 0)\n",
    "    windows = windows.reshape(windows.shape[0], windows.shape[1], windows.shape[2], 1)\n",
    "    \n",
    "    # Predict\n",
    "    predictions = model.predict(windows)\n",
    "    \n",
    "    # Find P and S arrivals\n",
    "    p_probs = predictions[:, 1]\n",
    "    s_probs = predictions[:, 2]\n",
    "    \n",
    "    p_window_idx = np.argmax(p_probs)\n",
    "    s_window_idx = np.argmax(s_probs)\n",
    "    \n",
    "    return {\n",
    "        'p_arrival': p_window_idx * 1500,  # window overlap\n",
    "        's_arrival': s_window_idx * 1500,\n",
    "        'p_confidence': float(p_probs[p_window_idx]),\n",
    "        's_confidence': float(s_probs[s_window_idx])\n",
    "    }\n",
    "\n",
    "print(\"âœ… Function ready for inference!\")\n",
    "print(\"\\nContoh penggunaan:\")\n",
    "print(\"results = predict_seismic_phases('your_waveform.csv')\")\n",
    "print(\"print(results)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## ðŸ“ Summary\n",
    "\n",
    "### âœ… Yang Telah Dibuat:\n",
    "\n",
    "1. **Dataset**: Synthetic seismic waveforms dengan P/S arrivals\n",
    "2. **Preprocessing**: Detrending, filtering (1-20 Hz), normalisasi\n",
    "3. **Augmentasi**: Noise, amplitude scaling, time shift\n",
    "4. **Model CNN**: 4 blok konvolusi + attention + dense layers\n",
    "5. **Training**: Dengan early stopping dan learning rate scheduling\n",
    "6. **Validasi**: STA/LTA comparison dan confusion matrix\n",
    "7. **Visualisasi**: Waveform + P/S detection seperti contoh\n",
    "\n",
    "### ðŸ“Š Output Files:\n",
    "- `best_model.h5` - Model terbaik saat training\n",
    "- `seismic_cnn_picker_final.h5` - Model final\n",
    "- `training_history.png` - Plot loss dan accuracy\n",
    "- `confusion_matrix.png` - Confusion matrix\n",
    "- `test_waveform_detection.png` - Visualisasi seperti contoh\n",
    "- `model_metadata.json` - Metadata model\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. Upload dataset CSV seismik Indonesia yang sebenarnya\n",
    "2. Fine-tune hyperparameters (learning rate, batch size, dll)\n",
    "3. Experiment dengan arsitektur lain (U-Net, ResNet)\n",
    "4. Implementasi regression model untuk arrival time yang lebih presisi\n",
    "5. Deploy model untuk production use\n",
    "\n",
    "---\n",
    "\n",
    "**Dibuat dengan â¤ï¸ untuk analisis seismik Indonesia**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
